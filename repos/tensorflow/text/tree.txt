|-- CONTRIBUTING.md
|-- LICENSE
|-- README.md
|-- WORKSPACE
|-- docs
|  |-- _book.yaml
|  |-- api_docs
|  |  |-- python
|  |  |  |-- _toc.yaml
|  |  |  |-- index.md
|  |  |  |-- text.md
|  |  |  |-- text
|  |  |  |  |-- BertTokenizer.md
|  |  |  |  |-- Detokenizer.md
|  |  |  |  |-- FirstNItemSelector.md
|  |  |  |  |-- HubModuleSplitter.md
|  |  |  |  |-- HubModuleTokenizer.md
|  |  |  |  |-- MaskValuesChooser.md
|  |  |  |  |-- RandomItemSelector.md
|  |  |  |  |-- Reduction.md
|  |  |  |  |-- RegexSplitter.md
|  |  |  |  |-- RoundRobinTrimmer.md
|  |  |  |  |-- SentencepieceTokenizer.md
|  |  |  |  |-- SplitMergeFromLogitsTokenizer.md
|  |  |  |  |-- SplitMergeTokenizer.md
|  |  |  |  |-- Splitter.md
|  |  |  |  |-- SplitterWithOffsets.md
|  |  |  |  |-- StateBasedSentenceBreaker.md
|  |  |  |  |-- Tokenizer.md
|  |  |  |  |-- TokenizerWithOffsets.md
|  |  |  |  |-- UnicodeCharTokenizer.md
|  |  |  |  |-- UnicodeScriptTokenizer.md
|  |  |  |  |-- WaterfallTrimmer.md
|  |  |  |  |-- WhitespaceTokenizer.md
|  |  |  |  |-- WordShape_cls.md
|  |  |  |  |-- WordpieceTokenizer.md
|  |  |  |  |-- _api_cache.json
|  |  |  |  |-- _toc.yaml
|  |  |  |  |-- all_symbols.md
|  |  |  |  |-- case_fold_utf8.md
|  |  |  |  |-- coerce_to_structurally_valid_utf8.md
|  |  |  |  |-- combine_segments.md
|  |  |  |  |-- find_source_offsets.md
|  |  |  |  |-- gather_with_default.md
|  |  |  |  |-- greedy_constrained_sequence.md
|  |  |  |  |-- keras.md
|  |  |  |  |-- keras
|  |  |  |  |  |-- layers.md
|  |  |  |  |  |-- layers
|  |  |  |  |  |  |-- ToDense.md
|  |  |  |  |-- mask_language_model.md
|  |  |  |  |-- max_spanning_tree.md
|  |  |  |  |-- max_spanning_tree_gradient.md
|  |  |  |  |-- metrics.md
|  |  |  |  |-- metrics
|  |  |  |  |  |-- rouge_l.md
|  |  |  |  |-- ngrams.md
|  |  |  |  |-- normalize_utf8.md
|  |  |  |  |-- normalize_utf8_with_offsets_map.md
|  |  |  |  |-- pad_along_dimension.md
|  |  |  |  |-- pad_model_inputs.md
|  |  |  |  |-- regex_split.md
|  |  |  |  |-- regex_split_with_offsets.md
|  |  |  |  |-- sentence_fragments.md
|  |  |  |  |-- sliding_window.md
|  |  |  |  |-- span_alignment.md
|  |  |  |  |-- span_overlaps.md
|  |  |  |  |-- viterbi_constrained_sequence.md
|  |  |  |  |-- wordshape.md
|  |-- guide
|  |  |-- BUILD
|  |  |-- _toc.yaml
|  |  |-- bert_preprocessing_guide.ipynb
|  |  |-- decoding_api.ipynb
|  |  |-- images
|  |  |  |-- embedding.jpg
|  |  |  |-- embedding2.png
|  |  |  |-- embeddings_classifier_accuracy.png
|  |  |  |-- one-hot.png
|  |  |-- subwords_tokenizer.ipynb
|  |  |-- tf_text_intro.md
|  |  |-- tokenizers.ipynb
|  |  |-- unicode.ipynb
|  |  |-- word_embeddings.ipynb
|  |-- include
|  |  |-- architecture-with-tf-text.png
|  |  |-- architecture-without-tf-text.png
|  |  |-- architecture.gif
|  |  |-- default.css
|  |  |-- tftext.png
|  |-- tutorials
|  |  |-- BUILD
|  |  |-- _toc.yaml
|  |  |-- bert_glue.ipynb
|  |  |-- classify_text_with_bert.ipynb
|  |  |-- fine_tune_bert.ipynb
|  |  |-- images
|  |  |  |-- attention_equation_1.jpg
|  |  |  |-- attention_equation_2.jpg
|  |  |  |-- attention_equation_3.jpg
|  |  |  |-- attention_equation_4.jpg
|  |  |  |-- bidirectional.png
|  |  |  |-- layered_bidirectional.png
|  |  |  |-- text_generation_sampling.png
|  |  |  |-- text_generation_training.png
|  |  |-- nmt_with_attention.ipynb
|  |  |-- text_classification_rnn.ipynb
|  |  |-- text_generation.ipynb
|  |  |-- text_similarity.ipynb
|  |  |-- transformer.ipynb
|  |  |-- uncertainty_quantification_with_sngp_bert.ipynb
|-- examples
|  |-- keras_example_174.ipynb
|-- oss_scripts
|  |-- build_docs.py
|  |-- configure.sh
|  |-- model_server
|  |  |-- save_models.py
|  |-- pip_package
|  |  |-- BUILD
|  |  |-- LICENSE
|  |  |-- MANIFEST.in
|  |  |-- build_pip_package.sh
|  |  |-- setup.nightly.py
|  |  |-- setup.py
|  |-- run_build.sh
|  |-- run_tests.sh
|-- tensorflow_text
|  |-- BUILD
|  |-- __init__.py
|  |-- core
|  |  |-- kernels
|  |  |  |-- BUILD
|  |  |  |-- LICENSE
|  |  |  |-- constrained_sequence.cc
|  |  |  |-- constrained_sequence.h
|  |  |  |-- constrained_sequence_kernel.cc
|  |  |  |-- constrained_sequence_kernel_input_validation_test.cc
|  |  |  |-- disjoint_set_forest.h
|  |  |  |-- disjoint_set_forest_test.cc
|  |  |  |-- edit_changes.proto
|  |  |  |-- exp_greedy_constrained_sequence_kernel_test.cc
|  |  |  |-- exp_viterbi_constrained_sequence_kernel_test.cc
|  |  |  |-- log_greedy_constrained_sequence_kernel_test.cc
|  |  |  |-- log_viterbi_constrained_sequence_kernel_test.cc
|  |  |  |-- mst_op_kernels.cc
|  |  |  |-- mst_solver.h
|  |  |  |-- mst_solver_random_comparison_test.cc
|  |  |  |-- mst_solver_test.cc
|  |  |  |-- normalize_kernels.cc
|  |  |  |-- normalize_kernels_test.cc
|  |  |  |-- regex_split.cc
|  |  |  |-- regex_split.h
|  |  |  |-- regex_split_kernels.cc
|  |  |  |-- regex_split_test.cc
|  |  |  |-- rouge_l_kernel.cc
|  |  |  |-- rouge_l_kernel_test.cc
|  |  |  |-- sentence_breaking_kernels.cc
|  |  |  |-- sentence_breaking_kernels_v2.cc
|  |  |  |-- sentence_breaking_utils.cc
|  |  |  |-- sentence_breaking_utils.h
|  |  |  |-- sentence_breaking_utils_test.cc
|  |  |  |-- sentence_fragmenter.cc
|  |  |  |-- sentence_fragmenter.h
|  |  |  |-- sentence_fragmenter_v2.cc
|  |  |  |-- sentence_fragmenter_v2.h
|  |  |  |-- sentence_fragmenter_v2_test.cc
|  |  |  |-- sentencepiece_kernels.cc
|  |  |  |-- spanning_tree_iterator.cc
|  |  |  |-- spanning_tree_iterator.h
|  |  |  |-- spanning_tree_iterator_test.cc
|  |  |  |-- split_merge_tokenize_kernel.cc
|  |  |  |-- text_kernels_test_util.cc
|  |  |  |-- text_kernels_test_util.h
|  |  |  |-- tokenizer_from_logits_kernel.cc
|  |  |  |-- unicode_script_tokenize_kernel.cc
|  |  |  |-- unicode_script_tokenize_kernel_test.cc
|  |  |  |-- whitespace_tokenize_kernel.cc
|  |  |  |-- whitespace_tokenize_kernel_test.cc
|  |  |  |-- wordpiece_kernel.cc
|  |  |  |-- wordpiece_kernel_test.cc
|  |  |  |-- wordpiece_tokenizer.cc
|  |  |  |-- wordpiece_tokenizer.h
|  |  |-- ops
|  |  |  |-- constrained_sequence_op.cc
|  |  |  |-- mst_ops.cc
|  |  |  |-- normalize_ops.cc
|  |  |  |-- regex_split_ops.cc
|  |  |  |-- rouge_l_op.cc
|  |  |  |-- sentence_breaking_ops.cc
|  |  |  |-- sentence_breaking_ops_v2.cc
|  |  |  |-- sentencepiece_ops.cc
|  |  |  |-- split_merge_tokenize_op.cc
|  |  |  |-- tokenizer_from_logits_op.cc
|  |  |  |-- unicode_script_tokenize_op.cc
|  |  |  |-- whitespace_tokenize_op.cc
|  |  |  |-- wordpiece_op.cc
|  |-- public_names_test.py
|  |-- python
|  |  |-- __init__.py
|  |  |-- benchmarks
|  |  |  |-- __init__.py
|  |  |  |-- benchmark_utils.py
|  |  |  |-- ops_benchmarks.py
|  |  |  |-- test_data
|  |  |  |  |-- uncased_L-12_H-768_A-12
|  |  |  |  |  |-- vocab.txt
|  |  |  |-- tokenizers_benchmarks.py
|  |  |-- keras
|  |  |  |-- __init__.py
|  |  |  |-- layers
|  |  |  |  |-- __init__.py
|  |  |  |  |-- todense.py
|  |  |  |  |-- todense_test.py
|  |  |-- metrics
|  |  |  |-- __init__.py
|  |  |  |-- text_similarity_metric_ops.py
|  |  |  |-- text_similarity_metric_ops_test.py
|  |  |-- numpy
|  |  |  |-- __init__.py
|  |  |  |-- viterbi_decode.py
|  |  |  |-- viterbi_decode_test.py
|  |  |-- ops
|  |  |  |-- __init__.py
|  |  |  |-- bert_tokenizer.py
|  |  |  |-- bert_tokenizer_test.py
|  |  |  |-- coerce_to_valid_utf8_op_test.py
|  |  |  |-- create_feature_bitmask_op.py
|  |  |  |-- create_feature_bitmask_op_test.py
|  |  |  |-- gather_with_default_op_test.py
|  |  |  |-- greedy_constrained_sequence_op.py
|  |  |  |-- greedy_constrained_sequence_op_test.py
|  |  |  |-- hub_module_splitter.py
|  |  |  |-- hub_module_splitter_test.py
|  |  |  |-- hub_module_tokenizer.py
|  |  |  |-- hub_module_tokenizer_test.py
|  |  |  |-- item_selector_ops.py
|  |  |  |-- item_selector_ops_test.py
|  |  |  |-- masking_ops.py
|  |  |  |-- masking_ops_test.py
|  |  |  |-- mst_ops.py
|  |  |  |-- mst_ops_test.py
|  |  |  |-- ngrams_op.py
|  |  |  |-- ngrams_op_test.py
|  |  |  |-- normalize_ops.py
|  |  |  |-- normalize_ops_test.py
|  |  |  |-- pad_along_dimension_op.py
|  |  |  |-- pad_along_dimension_op_test.py
|  |  |  |-- pad_model_inputs_ops.py
|  |  |  |-- pad_model_inputs_ops_test.py
|  |  |  |-- pointer_ops.py
|  |  |  |-- regex_split_ops.py
|  |  |  |-- regex_split_ops_test.py
|  |  |  |-- segment_combiner_ops.py
|  |  |  |-- segment_combiner_ops_test.py
|  |  |  |-- sentence_breaking_ops.py
|  |  |  |-- sentence_breaking_ops_test.py
|  |  |  |-- sentencepiece_tokenizer.py
|  |  |  |-- sentencepiece_tokenizer_test.py
|  |  |  |-- sliding_window_op.py
|  |  |  |-- sliding_window_op_test.py
|  |  |  |-- span_alignment_op_test.py
|  |  |  |-- span_overlaps_op_test.py
|  |  |  |-- split_merge_from_logits_tokenizer.py
|  |  |  |-- split_merge_from_logits_tokenizer_test.py
|  |  |  |-- split_merge_tokenizer.py
|  |  |  |-- split_merge_tokenizer_test.py
|  |  |  |-- splitter.py
|  |  |  |-- state_based_sentence_breaker_op.py
|  |  |  |-- state_based_sentence_breaker_op_test.py
|  |  |  |-- string_ops.py
|  |  |  |-- test_data
|  |  |  |  |-- segmenter_hub_module
|  |  |  |  |  |-- saved_model.pb
|  |  |  |  |  |-- tfhub_module.pb
|  |  |  |  |  |-- variables
|  |  |  |  |  |  |-- variables.data-00000-of-00001
|  |  |  |  |  |  |-- variables.index
|  |  |  |  |-- test_oss_model.model
|  |  |  |  |-- test_wp_en_vocab.txt
|  |  |  |-- tokenization.py
|  |  |  |-- trimmer_ops.py
|  |  |  |-- trimmer_ops_test.py
|  |  |  |-- unicode_char_tokenizer.py
|  |  |  |-- unicode_char_tokenizer_test.py
|  |  |  |-- unicode_script_tokenizer.py
|  |  |  |-- unicode_script_tokenizer_test.py
|  |  |  |-- viterbi_constrained_sequence_op.py
|  |  |  |-- viterbi_constrained_sequence_op_test.py
|  |  |  |-- whitespace_tokenizer.py
|  |  |  |-- whitespace_tokenizer_test.py
|  |  |  |-- wordpiece_tokenizer.py
|  |  |  |-- wordpiece_tokenizer_test.py
|  |  |  |-- wordshape_ops.py
|  |  |  |-- wordshape_ops_test.py
|  |-- tftext.bzl
|  |-- tools
|  |  |-- __init__.py
|  |  |-- wordpiece_vocab
|  |  |  |-- BUILD
|  |  |  |-- __init__.py
|  |  |  |-- bert_vocab_from_dataset.py
|  |  |  |-- bert_vocab_from_dataset_test.py
|  |  |  |-- generate_vocab.py
|  |  |  |-- generate_word_counts.py
|  |  |  |-- measure_wordpiece_stats.py
|  |  |  |-- utils.py
|  |  |  |-- utils_test.py
|  |  |  |-- wordpiece_tokenizer_learner.py
|  |  |  |-- wordpiece_tokenizer_learner_lib.py
|  |  |  |-- wordpiece_tokenizer_learner_test.py
|-- third_party
|  |-- icu
|  |  |-- BUILD
|  |  |-- BUILD.bzl
|  |  |-- data
|  |  |  |-- BUILD
|  |  |  |-- filters.json
|  |  |  |-- normalization_data.c
|  |  |-- udata.patch
|  |-- sentencepiece
|  |  |-- BUILD
|  |  |-- processor.patch
|  |-- tensorflow
|  |  |-- BUILD
|  |  |-- BUILD.tpl
|  |  |-- tf_configure.bzl
